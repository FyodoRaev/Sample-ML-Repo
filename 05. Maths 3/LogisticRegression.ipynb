{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obOtFyDKPuRG"
      },
      "source": [
        "Реализуем нашу логистическую регрессию.\n",
        "Для начала импортируем все нужные библиотеки, в частности sklearn с данными.\n",
        "\n",
        "\n",
        "```Python\n",
        "import numpy as np \n",
        "from numpy import log,dot,e,shape\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split  \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6gGgEOwQB1t"
      },
      "source": [
        "Создадим датасет, разобьем его на тестовую и тренировочную части.\n",
        "\n",
        "\n",
        "```python\n",
        "X,y = make_classification(n_features = 4,n_classes=2)\n",
        "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E7cUWu2QnY-"
      },
      "source": [
        "Теперь следует стандартизировать данные, ведь для некоторых моделей наличие нескольких фич сильно разной размерности, диапозона и единиц измерения является препятствием.\n",
        "Математически стандартизация будет реализована так: (x-μ)/σ.\n",
        "\n",
        "\n",
        "```python\n",
        "def standardize(self, X_tr):\n",
        "  for i in range(shape(X_tr)[1]):\n",
        "      X_tr[:,i] = (X_tr[:,i] - np.mean(X_tr[:,i]))/np.std(X_tr[:,i])\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "По сути мы делаем математическое ожидание аттрибута равным нулю и распределение имеет стандартное отклонение, равное нулю.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkFxH0SHTWKT"
      },
      "source": [
        "Инициализируем параметры, добавив в каждое наблюдение единичный bias. Bias важен, ведь он поможет сделать модель более гибкой.\n",
        "\n",
        "\n",
        "```python\n",
        "def initialize (self,X):\n",
        "      weights = np.zeros((shape(X)[1]+1,1))\n",
        "      X = np.c_[np.ones((shape(X)[0],1)),X]\n",
        "      return weights,X\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQA-kjgXUW_V"
      },
      "source": [
        "Модель логистической регрессии схожа с моделью линейной регрессии, ведь процесс создания предсказаний практически идентичен, за исключением того, что в логистической регресси к слою выхода применяется функция активации.\n",
        "Реализуем сигмоиду.\n",
        "![22366480px-Logistic-curve.svg.png](data:image/png;base64,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)\n",
        "Как можно заметить функция сигмоиды пересекает ось ординат в точке 0.5. Наше классификация будет считать, что если sigmoid(x)>0.5, то x∈FirstClass, иначе x∈SecondClass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24up5Zl-V8jP"
      },
      "source": [
        "Функция сигмоиды: sigmoid(x)= 1/(1+e^{-x}\n",
        "\n",
        "\n",
        "```python\n",
        "def sigmoid(self,x):\n",
        "    sigmoid = 1/(1+e**(-x))\n",
        "    return sigmoid\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Функция потерь\n",
        "\n",
        "Наша модель интепретирует скалярное произведение векторов $ w^Tx$ как вероятность, используя $sigmoid(x)$. Значит наше предсказание можно представлять как $ \\^y= P(y=1 |x  )$ \n",
        "\n",
        "Другой способ выразить то же самое выглядит так: \n",
        "* Если $y=1$:   $P(y| x)= \\^y$ \n",
        "* Если $y=0$:   $P(y| x) = 1 - \\^y$\n",
        "\n",
        "Тогда давайте объединим два этих выражения : $ P(y| x)=\\^y^y(1-\\^y)^{1-y}$\n",
        "Так как функция логарифма монотонно растет с ростом аргумента, максимизировав $log(P(y| x))$ мы максимизируем и $P(y| x)$. \n",
        "\n",
        "$f(\\^y,y)=logP(y|x)=y*log\\^y + (1-y)*log(1-\\^y)$\n",
        "\n",
        "Но так как функционал ошибок мы хотим минимизировать, то функция потерь будет равна $-f(\\^y,y)$ (minimazing the cost function == maximazing the log of the probability)\n",
        "\n",
        "Интуитивно, мы хотели бы назначать модели штраф за предсказание 1, когда на самом деле ответ 0, и наоборот. Функция потерь логистической регрессии так и делает. \n",
        "\n",
        "$Cost(h_\\theta(x), y) = \\begin{cases}\n",
        "      -log(h_\\theta(x)) \\rightarrow if \\quad y=1 \\\\\n",
        "      -log(1-h_\\theta(x)) \\rightarrow if \\quad y=0\n",
        "    \\end{cases}\\,.$\n",
        "\n",
        "Однако же мы можем переписать эту системы в одну строчку для будущих вычислений: \n",
        "\n",
        "$Cost(h_\\theta(x),y) =-ylog(h_\\theta(x))-(1-y)log(h_\\theta(x)) $    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKiPrdKQXL-r"
      },
      "source": [
        "Наша функция потерь будет выглядеть следующим образом:\n",
        "\n",
        "$J(w)= -\\frac{1}{m} (y^T*log(h(Xw))+(1-y)^T*log(1-h(Xw)))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4H4gf0MZLxa"
      },
      "source": [
        "↿h(x) - sigmoid function↾\n",
        "\n",
        "\n",
        "```Python\n",
        "def cost(theta):\n",
        "    prediction = X@theta \n",
        "    cost = -(y.T@log(self.sigmoid(prediction)) + (1-y).T@log(1-self.sigmoid(prediction)))/len(y)\n",
        "    return cost\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtEcrKCaVDW"
      },
      "source": [
        "Реализуем градиентный спуск:\n",
        "* Продифференцируем функцию потерь :\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial{w}}\n",
        "= \\left(-\\frac{y}{y_{pred}} + \\frac{1-y}{1-y_{pred}}\\right)\\frac{\\partial{y_{pred}}}{\\partial{w}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{y_{pred}}}{\\partial{w}} = \\frac{1}{(1+e^{-\\langle x, w \\rangle})^2} e^{-\\langle x, w \\rangle} (-x) = -y_{pred}(1-y_{pred})x\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial{w}} = (y_{pred} - y) x\n",
        "$$\n",
        "\n",
        "* \n",
        "```python\n",
        "def fit(self,X,y,alpha=0.001,iter=400):\n",
        "        weights,X = self.initialize(X)\n",
        "        def cost(theta):\n",
        "            z = dot(X,theta)\n",
        "            cost = -((1-y).T@(log(1-self.sigmoid(z))) +  y.T@log(self.sigmoid(z)))/len(y)\n",
        "            return cost\n",
        "        cost_list = np.zeros(iter,)\n",
        "        for i in range(iter):\n",
        "            z = sigmoid(logit(X_train, self.w))\n",
        "            grad = np.dot(X_train.T, (z - y)) / len(y)\n",
        "\n",
        "            weights -= grad * lr\n",
        "            cost_list[i] = cost(weights)\n",
        "        self.weights = weights\n",
        "        return cost_list\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLqD1z_uc232"
      },
      "source": [
        "Предсказания модели:\n",
        "\n",
        "\n",
        "```python\n",
        "def predict(self,X):\n",
        "        z = dot(self.initialize(X)[1],self.weights)\n",
        "        lis = []\n",
        "        for i in self.sigmoid(z):\n",
        "            if i>0.5:\n",
        "                lis.append(1)\n",
        "            else:\n",
        "                lis.append(0)\n",
        "        return lis\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrvx04JQf0aT"
      },
      "source": [
        "Давайте соберем вcе вместе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "IxP8SEl-fa1e",
        "outputId": "6adfa356-9234-4484-9c95-9658be33fea5"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from numpy import log,dot,exp,shape\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "X,y = make_classification(n_features=4)\n",
        "from sklearn.model_selection import train_test_split\n",
        "random_seed =3407\n",
        "np.random.seed(random_seed) \n",
        "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)\n",
        "                                       \n",
        "def standartize(Matrice):\n",
        "    for i in range(shape(Matrice)[1]):\n",
        "        X_tr[:,i] = (X_tr[:,i] - np.mean(X_tr[:,i]))/np.std(X_tr[:,i])\n",
        "def F1_score(y,y_hat):\n",
        "    tp,tn,fp,fn = 0,0,0,0\n",
        "    for i in range(len(y)):\n",
        "        if y[i] == 1 and y_hat[i] == 1:\n",
        "            tp += 1\n",
        "        elif y[i] == 1 and y_hat[i] == 0:\n",
        "            fn += 1\n",
        "        elif y[i] == 0 and y_hat[i] == 1:\n",
        "            fp += 1\n",
        "        elif y[i] == 0 and y_hat[i] == 0:\n",
        "            tn += 1\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    f1_score = 2*precision*recall/(precision+recall)\n",
        "    return f1_score\n",
        "class LogidticRegression:\n",
        "    def sigmoid(self,z):\n",
        "        sig = 1/(1+exp(-z))\n",
        "        return sig\n",
        "    def initialize(self,X):\n",
        "        weights = np.zeros((shape(X)[1]+1,1))\n",
        "        X = np.c_[np.ones((shape(X)[0],1)),X]\n",
        "        return weights,X\n",
        "    def fit(self,X,y,alpha=0.001,iter=400):\n",
        "        weights,X = self.initialize(X)\n",
        "        def cost(theta):\n",
        "            z = dot(X,theta)\n",
        "            cost = -((1-y).T@(log(1-self.sigmoid(z))) +  y.T@log(self.sigmoid(z)))/len(y)\n",
        "            return cost\n",
        "        cost_list = np.zeros(iter,)\n",
        "        for i in range(iter):\n",
        "            weights = weights - alpha*dot(X.T,self.sigmoid(dot(X,weights))-np.reshape(y,(len(y),1)))\n",
        "            cost_list[i] = cost(weights)\n",
        "        self.weights = weights\n",
        "        return cost_list\n",
        "    def predict(self,X):\n",
        "        z = dot(self.initialize(X)[1],self.weights)\n",
        "        lis = []\n",
        "        for i in self.sigmoid(z):\n",
        "            if i>0.5:\n",
        "                lis.append(1)\n",
        "            else:\n",
        "                lis.append(0)\n",
        "        return lis\n",
        "standartize(X_tr)\n",
        "standartize(X_te)\n",
        "obj1 = LogidticRegression()\n",
        "model= obj1.fit(X_tr,y_tr)\n",
        "y_pred = obj1.predict(X_te)\n",
        "y_train = obj1.predict(X_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Давайте взглянем на результаты нашей модели с помощью f1:\n",
        "\n",
        "$ F1 = 2 * \\frac{precision*recall}{precision+recall}$\n",
        "\n",
        "Точность(precision) и полнота(recall) являются метриками, которые используются при оценке большей части алгоритмов извлечения информации. Иногда они используются сами по себе, иногда в качестве базиса для производных метрик, таких как F-мера или R-Precision. Суть точности и полноты очень проста.\n",
        "\n",
        "Точность системы в пределах класса – это доля документов действительно принадлежащих данному классу относительно всех документов которые система отнесла к этому классу. Полнота системы – это доля найденных классфикатором документов принадлежащих классу относительно всех документов этого класса в тестовой выборке."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9411764705882352\n",
            "0.923076923076923\n"
          ]
        }
      ],
      "source": [
        "f1_score_tr = F1_score(y_tr,y_train)\n",
        "f1_score_te = F1_score(y_te,y_pred)\n",
        "print(f1_score_tr)\n",
        "print(f1_score_te)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LogisticRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
